---
title: "Analysis on Adverts"
author: "Katheu"
date: "7/2/2021"
output: html_document
---


## Defining The Question

A Kenyan entrepreneur has created an online cryptography course and would want to advertise it on her blog. She currently targets audiences originating from various countries. In the past, she ran ads to advertise a related course on the same blog and collected data in the process. She would now like to employ your services as a Data Science Consultant to help her identify which individuals are most likely to click on her ads.

## Metric of Success

To be able to do extensive data cleaning and exploratory data analysis. I will do both univariate and bivariate data analysis on the dataset.

## Understanding the context

Clicks on adverts can help you understand how well your ad is appealing to people who see it. Highly targeted ads are more likely to receive clicks. This can help you gauge how enticing your ad is. In this case, it would help us know how many people would be interested in the online cryptography course through the number of clicks on our client's blog.

## Experimental Design

* Loading the dataset
* Performing data cleaning
* Exploratory Data Analysis
* Conclusion and recommendation

## Data Relevance
* Daily Time Spent on Site - Time spent per day on the blog
* Age - Age of the respondents
* Area Income - Income Distribution of the respondents' area
* Daily Internet Usage - How much internet is used on a daily
* Ad Topic Line - Topic of the advert
* City - City of respondents
* Male - gender of respondents; 1 if male and 0 if female.
* Country -country of respondents
* Time stamp - the time the data is recorded
* Clicked on Ad - whether the respondents click on the ads; 0 if they don't and 1 if they do.

## Loading the Dataset

```{r}
ad <- read.csv("C:/Users/hp/Downloads/advertising.csv")
```

### Previewing the top of our dataset

```{r}
head(ad)
```
### Previewing the tail of our dataset

```{r}
tail(ad)
```

## Cleaning Data

Finding the total missing values in our dataset.
```{r}
colSums(is.na(ad))
#There are no missing values in our dataset
```

Checking for duplicates across our rows.

```{r}
ad[duplicated(ad),]

#There are no duplicates in this dataset.
```

## Exploring the dataset
Checking the descriptive statistics of our dataset
```{r}

summary(ad)
```
Checking the structure of our dataframe
```{r}
str(ad)
```

## Checking for outliers

Checking for outliers in the dataset. These show a visual shape of our data distribution.

```{r}
boxplot(ad$Area.Income,
        main ="Area Income",
        col = "orange",
        border  = 'brown',
        horizontal = TRUE,
        notch = TRUE)
#There are a few outliers in the area income column.
```
```{r}
boxplot(ad$Daily.Time.Spent.on.Site,
        main ="Daily Time Spent on Site",
        col = "orange",
        border  = 'brown',
        horizontal = TRUE,
        notch = TRUE)
#There are no outliers in the daily time spent on site column. 
```
```{r}
boxplot(ad$Age,
        main ="Age",
        col = "orange",
        border  = 'brown',
        horizontal = TRUE,
        notch = TRUE)
#There are no outliers in the age column.
```
```{r}
boxplot(ad$Daily.Internet.Usage,
        main ="Daily Internet Usage",
        col = "orange",
        border  = 'brown',
        horizontal = TRUE,
        notch = TRUE)
#There are no outliers in the daily internet usage column
```

## Exploratory Data Analysis

### Univariate Analysis 

#### Measures of Central Tendency

Finding the mean of our numeric columns
```{r}
colMeans(ad[sapply(ad,is.numeric)])
```

Finding the median of our numeric columns
```{r}
ad_time_median <- median(ad$Daily.Time.Spent.on.Site)
print(ad_time_median)
```

```{r}
ad_age_median <- median(ad$Age)
ad_age_median
```

```{r}
ad_income_median <- median(ad$Area.Income)
ad_income_median
```

```{r}
ad_internet_usage_median <- median(ad$Daily.Internet.Usage)
ad_internet_usage_median
```
Finding the mode of our numeric columns. Let's create the mode function
```{r}
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]}
```

Finding the mode in the age column
```{r}

getmode(ad$Age)
getmode(ad$Daily.Time.Spent.on.Site)
getmode(ad$Area.Income)
getmode(ad$Daily.Internet.Usage)
getmode(ad$City)
getmode(ad$Ad.Topic.Line)
getmode(ad$Male)
getmode(ad$Country)
getmode(ad$Timestamp)

```

Finding the minimum values in our numeric columns

```{r}
min(ad$Age)
min(ad$Daily.Time.Spent.on.Site)
min(ad$Area.Income)
min(ad$Daily.Internet.Usage)
```

Finding the maximum values in our numeric columns
```{r}
max(ad$Age)
max(ad$Daily.Time.Spent.on.Site)
max(ad$Area.Income)
max(ad$Daily.Internet.Usage)

```

Finding the range in our numeric columns
```{r}
range(ad$Age)
range(ad$Daily.Time.Spent.on.Site)
range(ad$Area.Income)
range(ad$Daily.Internet.Usage)

```

* The youngest respondent is 19 and the oldest 61 years of age.
* The least time spent on her site is 32 minutes and the highest 91 minutes.
* The lowest income earner among the respondents earns 13,996 while the highest earns 79,484.
* Daily internet usage ranges from 104 - 269.



Getting the quantiles in our columns
```{r}
quantile(ad$Age)
quantile(ad$Daily.Time.Spent.on.Site)
quantile(ad$Area.Income)
quantile(ad$Daily.Internet.Usage)
```

Finding the variance of the numeric columns. This shows how the data values are dispersed around the mean.
```{r}
var(ad$Age)
var(ad$Daily.Time.Spent.on.Site)
var(ad$Area.Income)
var(ad$Daily.Internet.Usage)
```

Finding the standard deviation of our columns.
```{r}
sd(ad$Age)
sd(ad$Daily.Time.Spent.on.Site)
sd(ad$Area.Income)
sd(ad$Daily.Internet.Usage)
```

#### Frequency Distribution

Finding the Frequency Distribution in our age column

```{r}
table(ad$Age)
# Most respondents fall between theage bracket 25-42. The age with the highest number of readers is 31 which has a total of 61 people in total.
```

#### Histogram

Plotting histograms for our columns
```{r}
hist(ad$Age, col  = "Cyan")
#Most respondents fall in the age bracket 25-40.
```

```{r}
hist(ad$Area.Income, col = "Purple")
#The respondents mostly earn between 50K - 70K
```
```{r}
hist(ad$Daily.Time.Spent.on.Site, col = "gold")
```

```{r}
hist(ad$Daily.Internet.Usage, col = "pink")
```

### Bivariate Analysis

#### Ggplots



```{r}

library(ggplot2)

ggplot(data = ad, aes(x = Area.Income, fill = Clicked.on.Ad))+
        geom_histogram(bins  =20,col = "orange")+
        labs(title = "Income Distribution", x = "Area Income", y= "Frequency", fill = "Clicked on Ad")+ scale_color_brewer(
                palette = "Set1"
        )
```
```{r}
ggplot(data = ad, aes(x = Age, fill = Clicked.on.Ad))+
        geom_histogram(bins  =20,col = "orange")+
        labs(title = "Age Distribution", x = "Age", y= "Frequency", fill = "Clicked on Ad")+ scale_color_brewer(
                palette = "Set1"
        )
```
```{r}
ggplot(data = ad, aes(x =Daily.Time.Spent.on.Site, fill = Clicked.on.Ad))+
        geom_histogram(bins  =20,col = "orange")+
        labs(title = "Daily Time Spent on Site", x = "Time Spent on Site", y= "Frequency", fill = "Clicked on Ad")+ scale_color_brewer(
                palette = "Set1"
        )
```


#### Covariance

Covariance is a statistical representation of the degree to which two variables vary together.

```{r}
cov(ad$Age, ad$Daily.Time.Spent.on.Site)
#There is a negative relationship between the age and the time spent on site which means as the age increases, the daily time spent on the site decreases. The opposite is true.
```
```{r}
cov(ad$Age, ad$Daily.Internet.Usage)

#There is a negative relationship between the age and the daily internet usage as well.
```
```{r}
cov(ad$Area.Income,ad$Daily.Time.Spent.on.Site)

#There is a strong positive relationship between the income and daily time spent on site variables. That goes to say that the higher the income, the more the time spent on site and the lower the income, the less the time spent on site.
```
```{r}
cov(ad$Age,ad$Area.Income)
#There is a negative correlation between the age and income variables.
```


#### Correlation matrix

```{r}
cor(ad$Age, ad$Daily.Time.Spent.on.Site)
cor(ad$Age,ad$Daily.Internet.Usage)
cor(ad$Area.Income,ad$Daily.Internet.Usage)
cor(ad$Area.Income,ad$Daily.Time.Spent.on.Site)
cor(ad$Age,ad$Area.Income)
```
```{r}
cor(ad[, c("Age","Daily.Time.Spent.on.Site","Daily.Internet.Usage")])
```


```{r}
cor(ad[,unlist(lapply(ad, is.numeric))])
```

#### Scatter Plots

Scatter plots are used when we want to see a graphical representation of two different variables. They show how the variables are correlated.

Let's plot a scatter plot for age and daily time spent on site.
```{r}
ggplot(ad, aes(Age, Daily.Time.Spent.on.Site))+
  geom_point()+
  labs(title = "Scatter Plot of Age Distribution vs Time Spent on Site",
       x = "Age",
       y = "Time Spent on Site")
```

Scatter plot for Income Distribution and Daily time spent on site.
```{r}
ggplot(ad, aes(Daily.Time.Spent.on.Site, Area.Income))+
  geom_point()+
  labs(title = "Time spent on site vs Income",
       x = "Daily Time Spent on Site",
       y = "Income Distribution")
```

Scatter plot for Age and Income Distribution
```{r}
ggplot(ad, aes(Age, Daily.Time.Spent.on.Site))+
  geom_point()+
  labs(title = "Scatter Plot of Age Distribution vs Time Spent on site",
       x = "Age",
       y = "Daily Time Spent on Site")
```
Scatter plot for Income and Daily Internet Usage

```{r}
ggplot(ad, aes(Area.Income, Daily.Internet.Usage))+
  geom_point()+
  labs(title = "Scatter Plot of Income Distribution vs Daily Internet Usage",
       x = "Income Distribution",
       y = "Daily Internet Usage")
```

## Modelling

### Multiple Linear Regression

Since we'd already created the scatter plots and established the relationship between the variables, we will dive straight into modelling.

Defining the variables we'll use in this model.


```{r}
input <- ad[,c("Clicked.on.Ad","Daily.Time.Spent.on.Site", "Age","Area.Income","Daily.Internet.Usage")]
```

```{r}
head(input)
```

Applying the lm() function

```{r}

multiple_lm <- lm(Clicked.on.Ad ~ Daily.Time.Spent.on.Site + Age + Area.Income + Daily.Internet.Usage, data = input)
multiple_lm
```
Let's assess our model.

```{r}
summary(multiple_lm)
```

```{r}
library(broom)

tidy(multiple_lm)
```

Checking our model's confidence intervals.
```{r}
library(MASS)
confint(multiple_lm)
```

Generating the ANOVA table
```{r}
anova(multiple_lm)
```

Predicting the response variable
```{r}
pred <- predict(multiple_lm, input)
pred
```

Let's cross validate our multiple linear regression model.
```{r}
library(caret)

multiple_lm2 <- train(Clicked.on.Ad ~ Daily.Time.Spent.on.Site + Age + Area.Income + Daily.Internet.Usage, data = input,
               method = "lm", 
               trControl = trainControl(method = "cv", 
                                        number = 10, 
                                        verboseIter = FALSE))
summary(multiple_lm2)

multiple_lm2
```
Our model has an RMSE value of 0.2088.

Let's use this train object as input to the predict method

```{r}
pred2 <- predict(multiple_lm2, input)
pred2

error <- pred2 - ad$Clicked.on.Ad
error

rmse_xval <- sqrt(mean(error^2))

rmse_xval
```

### K-Nearest Neighbours

The features in our dataset have very different ranges when compared to other features. If the distance formula was applied to unmodified features, there is a potential for the features with larger ranges to dominate or mask the features with smaller ranges. Because of this, it is important to prepare the data with feature scaling.

```{r}
#Randomizing our data for better results
random <- runif(1000, 1:4)
ad_random <- ad[order(random),]

# Selecting the first 6 rows from ad_random
head(ad_random)
```
Let's normalize our dataset. We will use the Min-Max Normalization

```{r}
normal <- function(x) (
  return( ((x - min(x)) /(max(x)-min(x))) )
)
normal(1:4)
ad_new <- as.data.frame(lapply(ad_random[1:4], normal))
summary(ad_new)
```

Let's create test and train datasets
```{r}
train <- ad_new[1:800,]
test <- ad_new[801:1000,]
train_sp <- ad_random[1:800,10]
test_sp <- ad_random[801:1000,10]
```

Let's call the class package which contains the KNN algorithm. The table(test_sp, model) is our confusion matrix. 

```{r}
library(class)    
require(class)

model <- knn(train= train,test=test, ,cl= train_sp, k=10)
table(factor(model))
table(test_sp,model)
```

Our confusion matrix predicted 190 observations out of 200 correctly giving us an accuracy of 95%.

### Decision Trees

Decision trees make a complex decision simpler by breaking it down into smaller, simpler decisions using divide-and-conquer strategy. They basically identify a set of if-else conditions that split data according to the value if the features. Let's implement the decision tree model into our dataset and see how it performs.
Let's call the libraries we need. 

```{r}
library(rpart.plot)
library(mlbench)
library(rpart)
```


```{r}
dt <- rpart(Clicked.on.Ad ~ Daily.Time.Spent.on.Site + Age + Area.Income + Daily.Internet.Usage, data = input, method  ="class")
rpart.plot(dt)         
```
Looking for feature importances.

```{r}
data.frame(dt$variable.importance)
```


```{r}
pr <- predict(dt, input, type = "class")
table(pr, ad$Clicked.on.Ad)

```

This decision tree algorithm predicts 957 correct observations out of 1000. This model achieves an accuracy of 95.7 %.

Let's train our model.
```{r}

library(caret)
set.seed(12)

model <- train(Clicked.on.Ad ~ Daily.Time.Spent.on.Site + Age + Area.Income + Daily.Internet.Usage ,data = input,method = "ranger") 
model
```

```{r}
plot(model)
```

### Support Vector Machines


```{r}
library(caret)
intrain <- createDataPartition(y = ad$Clicked.on.Ad, p= 0.7, list = FALSE)
training <- ad[intrain,]
testing <- ad[-intrain,]
```

Checking the dimension of the training and testing dataframe.

```{r}
dim(training);
dim(testing);
```

Let's factorize our target variable for accurate results.

```{r}
training[["Clicked.on.Ad"]] = factor(training[["Clicked.on.Ad"]])
```

Controlling the computational overheads using the trainControl() method.

```{r}

trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
```


```{r}
svm_Linear <- train(Clicked.on.Ad ~ Daily.Time.Spent.on.Site + Age + Area.Income +Daily.Internet.Usage , data = training, method = "svmLinear",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
```

Checking the result of our training model.

```{r}
svm_Linear
```
Predicting our model results using the predict() method.

```{r}
test_pred <- predict(svm_Linear, newdata = testing)
test_pred
```

Checking the accuracy of our model using a confusion matrix.

```{r}
confusionMatrix(table(test_pred, testing$Clicked.on.Ad))
```
Our model achieves an accuracy level of 96.95% which is pretty good.


## Conclusion and Recommendations

* Since the data shows that most of the respondents fall in the age bracket 25-41, she should tailor make the course to attract or suit more people in that age bracket. Also prudent to note that the youngest respondent is 19 and the oldest 61.
* Our client should target people with an income of 50,000 to 70,000 since those are the ones who seem interested and are in a position to afford the course.
* Most people spent about 70-85 minutes on the site so she should ensure that her course takes about the same time or even shorter per day so as to keep people interested.
* The client should implement the SVM model with this dataset because it has the highest accuracy.
* When she's coming up with the blog strategy, our client should prioritize the following features because they are the most important to figure out which clients will click on her adverts: Daily Internet Usage
                Daily Time Spent on Site
                Age
                Income distribution of the area.